{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mytransformer\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# NOTE: What is the differnece in the decoder block that was mentioned in the Random Transformer during inference and during training?\n",
    "# TODO: Find out if it has something to do with masked multihead attention, what exactly and if there is something else, what is? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Now we are ready to implement both the encoder and decoder blocks with the positional encoding to generate an output sequence. \n",
    "\n",
    "The complete transformer is made of two parts:\n",
    "- Encoder which takes the input sequence and generates a rich reperesentation. Composed of multiple stacks of encoder blocks.\n",
    "- Decoder which takes the generated encoder's output and generated tokens to generate the output sequence. It is also composed of stacks of decoder blocks. \n",
    "\n",
    "A final linear layer with a softmax on top of the decoder is necessary for word generation.\n",
    "\n",
    "The whole algorithm looks like this:\n",
    "1. Encoder processing: the encoder receives the input sequence (embedding with added positional encodings) and generates a rich representation which gets fed into the decoder. \n",
    "2. Decoder initialization: The decoding process begins with the start-of-sequence (SOS) token combined with the encoder's output.\n",
    "3. Decoder operation: the decoder uses the encoder's output together with all of the previously generated tokens to produce a new list of embeddings. \n",
    "4. Linear Layer for logits: a linear layer is applied to the last output embedding from the decoder to generate logits, representing raw predictions for the next token.\n",
    "5. Softmax for probabilities: These logits are passed through a softmax layer which converts them into a probability distribution over potential next tokens. \n",
    "6. Iterative token generation: this process is repeated with each step involving the cumulative embeddings of the previously generated tokens and the **initial** encoder's output.\n",
    "7. Sequence completion: The generation continues through these steps until the end-of-sequence (EOS) token is produced or a predefined sequence length is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Layer\n",
    "\n",
    "This is a simple linear transformation that takes the decoder's output and transforms it into a vector of vocab_size. vocab_size is the size of our vocabulary. \n",
    "For our example, it will be made up of 10 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.75454625, -1.72984897, -2.74518706, -5.10065025, -1.06210493,\n",
       "         0.89496205, -1.94516827, -0.61045523,  2.2844327 ,  1.41214933]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear(x, W, b):\n",
    "    return (x @ W) + b\n",
    "\n",
    "# We assume our decoder's output is a simple vector [1, 0, 1, 0]\n",
    "logits = linear(x=np.array([[1,0,1,0]]), W=np.random.randn(4, 10), b=np.random.randn(1, 10))\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we use as input for the linear layer? The decoder will output one embedding for each token in the sequence. \n",
    "\n",
    "The input for the linear layer will be the last generated embedding. The last embedding encapsulates information to the entire sequence up to that point, so it contains all the information needed to generate the next token. \n",
    "\n",
    "\n",
    "**This means that each output embedding from the decoder contains information about the entire sequence up to that point.**\n",
    "\n",
    "### 2. Softmax\n",
    "\n",
    "The out of the linear layer and the input to the softmax layer are called logits and softmax is needed to obtain the word probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02594799, 0.00978442, 0.0035447 , 0.00033621, 0.019078  ,\n",
       "        0.13504427, 0.00788902, 0.02996965, 0.54189555, 0.22651018]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x: np.array) -> np.array:\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "probs = softmax(x=logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Random Encoder-Decoder Transformer\n",
    "\n",
    "Firstly, we need to define a dictionary that maps the words to their initial embeddings. \n",
    "Usually, word2vec or GlOVE is used, but I am using random initializations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': array([ 0.77951459,  0.75806863, -1.08660408, -0.33525287]),\n",
       " 'mundo': array([ 0.09980578,  0.14668742, -0.1784667 , -0.2883775 ]),\n",
       " 'world': array([-1.63377171, -0.69276119,  1.18119715, -0.17603885]),\n",
       " 'how': array([-1.33615235, -1.10483817, -0.06466068,  0.39783812]),\n",
       " '?': array([ 0.34698074, -0.86457566,  1.39360451, -0.91708825]),\n",
       " 'EOS': array([-0.65274899, -0.12009902, -0.45978305,  0.76393155]),\n",
       " 'SOS': array([ 0.60378661,  0.62930697, -0.55687916, -0.15650524]),\n",
       " 'a': array([ 0.81077324, -0.25395342, -0.30993164, -0.06185999]),\n",
       " 'hola': array([-1.27392534,  1.36883098, -0.45121209, -0.26708426]),\n",
       " 'c': array([ 0.29710762, -0.3372509 , -0.65838874,  0.79640235])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\n",
    "    \"hello\",\n",
    "    \"mundo\",\n",
    "    \"world\",\n",
    "    \"how\",\n",
    "    \"?\",\n",
    "    \"EOS\",\n",
    "    \"SOS\",\n",
    "    \"a\",\n",
    "    \"hola\",\n",
    "    \"c\",\n",
    "]\n",
    "embedding_reps = np.random.randn(10, 4)\n",
    "vocabulary_embeddings = {\n",
    "    word: embedding_reps[i] for i, word in enumerate(vocabulary)\n",
    "}\n",
    "vocabulary_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a generate function that takes in the input sequence and generates tokens autoregressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_sequence, max_iters=3):\n",
    "    # Firstly, we encode the inputs into embeddings \n",
    "    embedded_inputs = [\n",
    "        vocabulary_embeddings[token] for token in input_sequence\n",
    "    ]\n",
    "\n",
    "    print(f\"Embedded representations (encoder input):\\n{embedded_inputs}\")\n",
    "\n",
    "    # NOTE: (Apparently not) Next, we need to positionaly encode each embedding\n",
    "    encoder_output = mytransformer.encoder(x=embedded_inputs)\n",
    "    print(f\"Embedding generated by encoder (encoder output):\\n{encoder_output}\")\n",
    "\n",
    "    # We initialize the decoder output with the embedding of the start token\n",
    "    sequence_embeddings = [vocabulary_embeddings[\"SOS\"]]\n",
    "    output = \"SOS\"\n",
    "\n",
    "    # Random matrices for the linear layer\n",
    "    d_vocab = len(vocabulary_embeddings)\n",
    "    W = np.random.randn(mytransformer.d_embedding, d_vocab)\n",
    "    b = np.random.randn(1, d_vocab)\n",
    "    # logits = linear(x=sequence_embeddings, )\n",
    "\n",
    "    # We limit number of decoding steps to avoid too long sequences without EOS\n",
    "    for i in range(max_iters):\n",
    "        # Decoder step\n",
    "        decoder_output = mytransformer.decoder(x=sequence_embeddings, decoder_embedding=encoder_output)\n",
    "\n",
    "        # Only the last output is for prediction (as that token contains all the necessary information of the previously generated tokens)\n",
    "        logits = linear(decoder_output[-1], W, b)        \n",
    "\n",
    "        # Pass it through the softmax layer\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        # We then generate the most likely next token\n",
    "        next_token = vocabulary[np.argmax(probs)]\n",
    "        sequence_embeddings.append(vocabulary_embeddings[next_token])\n",
    "        output += \" \" + next_token\n",
    "\n",
    "        print(f\"\"\"\n",
    "            Iteration: {i}\n",
    "            Generated token: {next_token}\n",
    "            Token probability: {np.max(probs)}\n",
    "        \"\"\")\n",
    "\n",
    "        # If the end-of-sequence token is generated, we return the sequence and end the generation\n",
    "        if next_token == \"EOS\":\n",
    "            return output\n",
    "        \n",
    "    return output, sequence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded representations (encoder input):\n",
      "[array([ 0.77951459,  0.75806863, -1.08660408, -0.33525287]), array([-1.63377171, -0.69276119,  1.18119715, -0.17603885])]\n",
      "Embedding generated by encoder (encoder output):\n",
      "[[ 0.41465919 -1.2092834   1.40973063 -0.61510241]\n",
      " [ 0.41304152 -1.20857394  1.41076047 -0.61522406]]\n",
      "\n",
      "            Iteration: 0\n",
      "            Generated token: a\n",
      "            Token probability: 0.5183529832584921\n",
      "        \n",
      "\n",
      "            Iteration: 1\n",
      "            Generated token: ?\n",
      "            Token probability: 0.9852352948654279\n",
      "        \n",
      "\n",
      "            Iteration: 2\n",
      "            Generated token: ?\n",
      "            Token probability: 0.6350116052193634\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SOS a ? ?',\n",
       " [array([ 0.60378661,  0.62930697, -0.55687916, -0.15650524]),\n",
       "  array([ 0.81077324, -0.25395342, -0.30993164, -0.06185999]),\n",
       "  array([ 0.34698074, -0.86457566,  1.39360451, -0.91708825]),\n",
       "  array([ 0.34698074, -0.86457566,  1.39360451, -0.91708825])])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate([\"hello\", \"world\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
