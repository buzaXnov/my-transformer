{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence: str = \"Hello World\"\n",
    "# The original transformer method used a vector of size 512, I will do 4.\n",
    "# The embedding is random for now and I will hardcode a matrix. I might apply word2vec or GloVe later.\n",
    "E = np.array([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5]\n",
    "], dtype=np.float64)\n",
    "d_model = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embeddings: np.array, verbose: bool = False) -> np.array:\n",
    "    mat = np.zeros_like(a=embeddings, dtype=embeddings.dtype)\n",
    "    for pos, embedding in enumerate(embeddings):\n",
    "        if verbose:\n",
    "            print(f\"\\nEmbedding {embedding}\")\n",
    "        # for i, _ in enumerate(embedding): \n",
    "        # Pos encoding should be word agnostic and only looking at the \n",
    "        # position in the embedding matrix pos and dimension inside each embedding i.\n",
    "        for i in range(d_model):\n",
    "            func = np.sin if not i % 2 else np.cos\n",
    "            mat[pos][i] = func(pos / 10000 ** ((2 * i) / d_model))\n",
    "\n",
    "            # Copilot generated print horror for sanity check\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"i = {i} ({'even' if i % 2 == 0 else 'odd'}): PE({pos},{i}) = sin({pos} / 10000^({2 * i} / {d_model})) = sin({pos / 10000 ** ((2 * i) / d_model)}) = {func(pos / 10000 ** ((2 * i) / d_model))}\"\n",
    "                )\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 1.        ],\n",
       "       [0.84147098, 0.99995   , 0.0001    , 1.        ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encodings = positional_encoding(E)\n",
    "positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding positional encoding and input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E before adding:\n",
      "[[1. 2. 3. 4.]\n",
      " [2. 3. 4. 5.]]\n",
      "\n",
      "Positional encodings:\n",
      "[[0.         1.         0.         1.        ]\n",
      " [0.84147098 0.99995    0.0001     1.        ]]\n",
      "\n",
      "E after adding:\n",
      "[[1.         3.         3.         5.        ]\n",
      " [2.84147098 3.99995    4.0001     6.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"E before adding:\\n{E}\")\n",
    "print(f\"\\nPositional encodings:\\n{positional_encodings}\")\n",
    "\n",
    "E = E + positional_encodings\n",
    "\n",
    "print(f\"\\nE after adding:\\n{E}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention\n",
    "Let’s use 2 attention heads for our example. We’ll use random values for these matrices. Each matrix will be a 4x3 matrix. \n",
    "\n",
    "With this, each matrix will transform the 4-dimensional embeddings into 3-dimensional keys, values, and queries. \n",
    "\n",
    "This reduces the dimensionality for attention mechanism, which helps in managing the computational complexity. \n",
    "\n",
    "Note that using a too small attention size will hurt the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WV1 = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WQ1 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "\n",
    "WK2 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 1, 0]])\n",
    "WV2 = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]])\n",
    "WQ2 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to multiply our embeddings with the weight matrices to obtain the keys, queries and values.\n",
    "K1 = E @ WK1    # Key calculations\n",
    "V1 = E @ WV1    # Values calculations\n",
    "Q1 = E @ WQ1    # Query calculations\n",
    "\n",
    "K2 = E @ WK2\n",
    "V2 = E @ WV2\n",
    "Q2 = E @ WQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention function used in the paper is computed as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ is the matrix of queries\n",
    "- $K$ is the matrix of keys\n",
    "- $V$ is the matrix of values\n",
    "- $d_k$ is the dimension of the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
