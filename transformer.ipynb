{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence: str = \"Hello World\"\n",
    "# The original transformer method used a vector of size 512, I will do 4.\n",
    "# The embedding is random for now and I will hardcode a matrix. I might apply word2vec or GloVe later.\n",
    "E = np.array([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5]\n",
    "], dtype=np.float64)\n",
    "d_model = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embeddings: np.array, verbose: bool = False) -> np.array:\n",
    "    mat = np.zeros_like(a=embeddings, dtype=embeddings.dtype)\n",
    "    for pos, embedding in enumerate(embeddings):\n",
    "        if verbose:\n",
    "            print(f\"\\nEmbedding {embedding}\")\n",
    "        # for i, _ in enumerate(embedding): \n",
    "        # Pos encoding should be word agnostic and only looking at the \n",
    "        # position in the embedding matrix pos and dimension inside each embedding i.\n",
    "        for i in range(d_model):\n",
    "            func = np.sin if not i % 2 else np.cos\n",
    "            mat[pos][i] = func(pos / 10000 ** ((2 * i) / d_model))\n",
    "\n",
    "            # Copilot generated print horror for sanity check\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"i = {i} ({'even' if i % 2 == 0 else 'odd'}): PE({pos},{i}) = sin({pos} / 10000^({2 * i} / {d_model})) = sin({pos / 10000 ** ((2 * i) / d_model)}) = {func(pos / 10000 ** ((2 * i) / d_model))}\"\n",
    "                )\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 1.        ],\n",
       "       [0.84147098, 0.99995   , 0.0001    , 1.        ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encodings = positional_encoding(E)\n",
    "positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding positional encoding and input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E before adding:\n",
      "[[1. 2. 3. 4.]\n",
      " [2. 3. 4. 5.]]\n",
      "\n",
      "Positional encodings:\n",
      "[[0.         1.         0.         1.        ]\n",
      " [0.84147098 0.99995    0.0001     1.        ]]\n",
      "\n",
      "E after adding:\n",
      "[[1.         3.         3.         5.        ]\n",
      " [2.84147098 3.99995    4.0001     6.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"E before adding:\\n{E}\")\n",
    "print(f\"\\nPositional encodings:\\n{positional_encodings}\")\n",
    "\n",
    "E = E + positional_encodings\n",
    "\n",
    "print(f\"\\nE after adding:\\n{E}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention\n",
    "Let’s use 2 attention heads for our example. We’ll use random values for these matrices. Each matrix will be a 4x3 matrix. \n",
    "\n",
    "With this, each matrix will transform the 4-dimensional embeddings into 3-dimensional keys, values, and queries. \n",
    "\n",
    "This reduces the dimensionality for attention mechanism, which helps in managing the computational complexity. \n",
    "\n",
    "Note that using a too small attention size will hurt the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WV1 = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WQ1 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "\n",
    "WK2 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 1, 0]])\n",
    "WV2 = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]])\n",
    "WQ2 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# We need to multiply our embeddings with the weight matrices to obtain the keys, queries and values.\n",
    "K1 = E @ WK1    # Key calculations\n",
    "V1 = E @ WV1    # Values calculations\n",
    "Q1 = E @ WQ1    # Query calculations\n",
    "\n",
    "K2 = E @ WK2\n",
    "V2 = E @ WV2\n",
    "Q2 = E @ WQ2\n",
    "print(f\"Shape {K1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention function used in the paper is computed as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ is the matrix of queries\n",
    "- $K$ is the matrix of keys\n",
    "- $V$ is the matrix of values\n",
    "- $d_k$ is the dimension of the keys\n",
    "\n",
    "\n",
    "Calculating the attention score requires a couple of steps:\n",
    "\n",
    "1. Calculate the dot product of the query with each key\n",
    "2. Divide the result by the square root of the dimension of the key vector\n",
    "3. Apply a softmax function to obtain the attention weights\n",
    "4. Multiply each value vector by the attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68.        , 105.25713083],\n",
       "       [ 87.9998    , 135.78163588]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.2598183 , 60.77023282],\n",
       "       [50.80670822, 78.39356402]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "scores1 = scores1 / np.sqrt(K1.shape[1])\n",
    "scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is computed as:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is the i-th element of the input vector\n",
    "- The denominator is the sum of the exponentials of all elements in the input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3\n",
    "def softmax(mat: np.array) -> np.array:\n",
    "    return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n",
    "\n",
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reminding myself about the axis dilemma\n",
    "mat = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "print(mat)\n",
    "np.sum(mat, axis=None)\n",
    "# Axis 1 means that the second dimension of the shape of the matrix is going to be collapsed and the number in that direction will be summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.00005   , 8.84147098, 6.84157098],\n",
       "       [8.00005   , 8.84147098, 6.84157098]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4\n",
    "scores1 = scores1 @ V1\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(E: np.array, WQ: np.array, WK: np.array, WV: np.array) -> np.array:\n",
    "    Q = E @ WQ\n",
    "    K = E @ WK\n",
    "    V = E @ WV\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.00004391, 8.84146233, 6.84156233],\n",
       "       [8.00004981, 8.84147071, 6.84157071]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = attention(E, WQ1, WQ1, WV1)\n",
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.84147098, 3.99995   , 8.00005   ],\n",
       "       [8.84147098, 3.99995   , 8.00005   ]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we get the attention score for the second attention head.\n",
    "score2 = attention(E, WQ2, WQ2, WV2)\n",
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.00005   , 8.84147098, 6.84157098, 8.84147098, 3.99995   ,\n",
       "        8.00005   ],\n",
       "       [8.00005   , 8.84147098, 6.84157098, 8.84147098, 3.99995   ,\n",
       "        8.00005   ]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.concatenate((scores1, score2), axis=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally multiply this concatenated matrix by a weight matrix to obtain the final output of the attention layer. \n",
    "\n",
    "This weight matrix is also learned! \n",
    "\n",
    "The dimension of the matrix ensures we go back to the same dimension as the embedding (4 in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.97128599, -14.12917589, -12.49224156, -18.50167966],\n",
       "       [ 11.971286  , -14.12917589, -12.49224156, -18.50167966]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just some random values\n",
    "W = np.array(\n",
    "    [\n",
    "        [0.79445237, 0.1081456, 0.27411536, 0.78394531],\n",
    "        [0.29081936, -0.36187258, -0.32312791, -0.48530339],\n",
    "        [-0.36702934, -0.76471963, -0.88058366, -1.73713022],\n",
    "        [-0.02305587, -0.64315981, -0.68306653, -1.25393866],\n",
    "        [0.29077448, -0.04121674, 0.01509932, 0.13149906],\n",
    "        [0.57451867, -0.08895355, 0.02190485, 0.24535932],\n",
    "    ]\n",
    ")\n",
    "Z = scores @ W\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward layer\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "\n",
    "connected feed-forward network, which is applied to each position separately and identically. \n",
    "\n",
    "\n",
    "This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "The feed forward layer, denoted as FFN, is computed as:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input\n",
    "- $W_1$ and $W_2$ are the weight matrices\n",
    "- $b_1$ and $b_2$ are the bias vectors\n",
    "- $\\max(0, \\cdot)$ is the ReLU (Rectified Linear Unit) activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this approach, I will expand from 4 dimensions to 8 in the first layer and then back to 4 in the second. \n",
    "d_model = 8\n",
    "W1 = np.random.randn(4, d_model)\n",
    "b1 = np.random.randn(d_model)\n",
    "W2 = np.random.randn(d_model, 4)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.array) -> np.array:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def feed_forward(x: np.array) -> np.array:\n",
    "    return relu(x @ W1 + b1) @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.27537029,  5.73870724, 44.73625853, 33.08235132],\n",
       "       [21.2753703 ,  5.73870724, 44.73625853, 33.08235133]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encapsulating everything so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 4\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "\n",
    "def attention(E: np.array, WQ: np.array, WK: np.array, WV: np.array) -> np.array:\n",
    "    Q = E @ WQ\n",
    "    K = E @ WK\n",
    "    V = E @ WV\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V\n",
    "\n",
    "def multihead_attention(x, WQs, WKs, WVs) -> np.array:\n",
    "    attentions = np.concatenate([attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1)\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "def relu(x: np.array) -> np.array:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def feed_forward(x: np.array, W1: np.array, W2: np.array, b1: np.array, b2: np.array) -> np.array:\n",
    "    return relu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "def encoder_block(x: np.array, WQs: np.array, WKs: np.array, WVs: np.array, W1: np.array, W2: np.array, b1: np.array, b2: np.array) -> np.array:\n",
    "    Z = multihead_attention(x, WQs, WKs, WVs)\n",
    "    Z = feed_forward(Z, W1, W2, b1, b2)\n",
    "    return Z\n",
    "\n",
    "def random_encoder_block(x: np.array) -> np.array:\n",
    "    WQs = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "\n",
    "    return encoder_block(x, WQs, WKs, WVs, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 3.        , 3.        , 5.        ],\n",
       "       [2.84147098, 3.99995   , 4.0001    , 6.        ]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.65114892, -3.74230864, -1.65071085, -1.83249781],\n",
       "       [ 5.65115293, -3.74231196, -1.6507186 , -1.83251494]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_encoder_block(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just one encoder block. The original paper uses 6 encoders. The output of one encoder goes to the next, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x: np.array):\n",
    "    for _ in range(6):\n",
    "        x = random_encoder_block(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_232297/1530465706.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n",
      "/tmp/ipykernel_232297/1530465706.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting too big numbers thus we need to use normalization.\n",
    "There are two common techniques to mitigate this problem: residual connections and layer normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
