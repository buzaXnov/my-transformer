{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embeddings: np.array, d_model: int = 4, verbose: bool = False) -> np.array:\n",
    "    mat = np.zeros_like(a=embeddings, dtype=embeddings.dtype)\n",
    "    for pos, embedding in enumerate(embeddings):\n",
    "        if verbose:\n",
    "            print(f\"\\nEmbedding {embedding}\")\n",
    "        # for i, _ in enumerate(embedding): \n",
    "        # Pos encoding should be word agnostic and only looking at the \n",
    "        # position in the embedding matrix pos and dimension inside each embedding i.\n",
    "        for i in range(d_model):\n",
    "            func = np.sin if not i % 2 else np.cos\n",
    "            mat[pos][i] = func(pos / 10000 ** ((2 * i) / d_model))\n",
    "\n",
    "            # Copilot generated print horror for sanity check\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"i = {i} ({'even' if i % 2 == 0 else 'odd'}): PE({pos},{i}) = sin({pos} / 10000^({2 * i} / {d_model})) = sin({pos / 10000 ** ((2 * i) / d_model)}) = {func(pos / 10000 ** ((2 * i) / d_model))}\"\n",
    "                )\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence: str = \"Hello World\"\n",
    "# The original transformer method used a vector of size 512, I will do 4.\n",
    "# The embedding is random for now and I will hardcode a matrix. I might apply word2vec or GloVe later.\n",
    "E = np.array([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5]\n",
    "], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 3.        , 3.        , 5.        ],\n",
       "       [2.84147098, 3.99995   , 4.0001    , 6.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E + positional_encoding(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder block receives two inputs: the output of the encoder and the generated output sequence. The output of the encoder is the representation of the input sequence. During inference, the generated output sequence starts with a special start-of-sequence token (SOS). During training, the target output sequence is the actual output sequence, shifted by one position. This will be clearer soon!\n",
    "\n",
    "The decoder is autoregressive, that means that the decoder will take the previously generated tokens and again generate the second token.\n",
    "- Iteration 1: Input is SOS, output is “hola”\n",
    "- Iteration 2: Input is SOS + “hola”, output is “mundo”\n",
    "- Iteration 3: Input is SOS + “hola” + “mundo”, output is EOS\n",
    "\n",
    "Here, SOS is the start-of-sequence token and EOS is the end-of-sequence token. The decoder will stop when it generates the EOS token. It generates one token at a time. Note that all iterations use the embedding generated by the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This autoregressive design makes decoder slow. The encoder is able to generate its embedding in a single forward pass while **the decoder needs to do many forward passes**. This is one of the reasons why architectures that only use the encoder (such as BERT or sentence similarity models) are much faster than decoder-only architectures (such as GPT-2 or BART).\n",
    "\n",
    "## Text embedding and positional encoding\n",
    "The first text of the decoder is to embed the input tokens. The input token is SOS, so we’ll embed it. We’ll use the same embedding dimension as the encoder. Let’s assume the embedding vector for SOS is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = np.array([[1, 0, 0, 0]], dtype=np.float64)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0., 2.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = E + positional_encoding(E)\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 4\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "epsilon = 1e-6\n",
    "gamma = 1\n",
    "beta = 0\n",
    "\n",
    "def softmax(mat: np.array) -> np.array:\n",
    "    return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def attention(E: np.array, WQ: np.array, WK: np.array, WV: np.array) -> np.array:\n",
    "    Q = E @ WQ\n",
    "    K = E @ WK\n",
    "    V = E @ WV\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V\n",
    "\n",
    "\n",
    "def multihead_attention(x, WQs, WKs, WVs) -> np.array:\n",
    "    attentions = np.concatenate(\n",
    "        [attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "\n",
    "def relu(x: np.array) -> np.array:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def feed_forward(\n",
    "    x: np.array, W1: np.array, W2: np.array, b1: np.array, b2: np.array\n",
    ") -> np.array:\n",
    "    return relu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "\n",
    "def layer_norm(x: np.array, epsilon: float=1e-6) -> np.array:\n",
    "    mean = x.mean(axis=1, keepdims=True)\n",
    "    # var = x.var(axis=1, keepdims=True)  # NOTE: variance is the standard deviation squared\n",
    "    # To lower the amount of calcuations (avoid calc. var and then sqrt in the denominator) we calculate the std dev\n",
    "    std_dev = x.std(axis=1, keepdims=True)\n",
    "    return (x - mean) / std_dev + epsilon * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-21.50369539,   7.5471242 ,  -5.23365504,  -3.92245733]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "Z_self_attention = multihead_attention(E, WQs, WKs, WVs)\n",
    "Z_self_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Things are quite simple for inference. For training, things are a bit tricky. During training, we use unlabeled data: just a bunch of text data, frequentyl scraped from the web. While the encoder’s goal is to capture all information of the input, the decoder’s goal is to predict the most likely next token. This means that the decoder can only use the tokens that have been generated so far (it cannot cheat and see the next tokens).\n",
    "\n",
    "Because of this, we use masked self-attention: **we mask the tokens that have not been generated yet. This is done by setting the attention scores to -inf. This is done in the original paper (section 3.2.3.1)**. We’ll skip this for now, but it’s important to keep in mind that the decoder is a bit more complex during training.\n",
    "\n",
    "\n",
    "## Residual connection and normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51918584,  1.28731024,  0.05260473,  0.17927487]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_self_attention = layer_norm(Z_self_attention)\n",
    "Z_self_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the self-attention mechanism, we calculate the queries, keys, and values from the input embedding.\n",
    "\n",
    "\n",
    "In the encoder-decoder attention, we calculate the queries from the previous decoder layer and the keys and values from the encoder output! All the math is the same as before; the only difference is what embedding to use for the queries. Let’s look at some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder_attention(\n",
    "    encoder_output: np.array,\n",
    "    attention_input: np.array,\n",
    "    WQ: np.array,\n",
    "    WK: np.array,\n",
    "    WV: np.array,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    attention_input: the output of the previous attention!\n",
    "    \"\"\"\n",
    "    K = encoder_output @ WK\n",
    "    V = encoder_output @ WV\n",
    "    Q = attention_input @ WQ\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V\n",
    "\n",
    "\n",
    "def multihead_encoder_decoder_attention(\n",
    "    encoder_output: np.array,\n",
    "    attention_input: np.array,\n",
    "    WQs: np.array,\n",
    "    WKs: np.array,\n",
    "    WVs: np.array,\n",
    ") -> np.array:\n",
    "    attentions = np.concatenate(\n",
    "        [\n",
    "            encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV)\n",
    "            for WQ, WK, WV in zip(WQs, WKs, WVs)\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.10578179, 0.99425408, 7.74487914, 4.23812386]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "# We assume this is the encoder's output\n",
    "encoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]], dtype=np.float64)\n",
    "\n",
    "Z_encoder_decoder = multihead_encoder_decoder_attention(\n",
    "    encoder_output, Z_self_attention, WQs, WKs, WVs\n",
    ")\n",
    "\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that we want the decoder to focus on the relevant parts of the input text (e.g., “hello world”). The encoder-decoder attention allows each position in the decoder to attend over all positions in the input sequence. This is very helpful for tasks such as translation, where the decoder needs to focus on the relevant parts of the input sequence. The decoder will learn to focus on the relevant parts of the input sequence by learning to generate the correct output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.49096473, -0.94517828, -0.87899475,  0.3332123 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z_self_attention)\n",
    "Z_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71906955, -1.72527844,  0.51088732,  0.49532556]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = np.random.randn(4, 8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(4)\n",
    "\n",
    "output = layer_norm(feed_forward(Z_encoder_decoder, W1, W2, b1, b2) + Z_encoder_decoder)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encapsulating everything: The Random Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 4\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "epsilon = 1e-6\n",
    "gamma = 1\n",
    "beta = 0\n",
    "\n",
    "\n",
    "def softmax(mat: np.array) -> np.array:\n",
    "    return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def attention(E: np.array, WQ: np.array, WK: np.array, WV: np.array) -> np.array:\n",
    "    Q = E @ WQ\n",
    "    K = E @ WK\n",
    "    V = E @ WV\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V\n",
    "\n",
    "\n",
    "def multihead_attention(x, WQs, WKs, WVs) -> np.array:\n",
    "    attentions = np.concatenate(\n",
    "        [attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "\n",
    "def relu(x: np.array) -> np.array:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def feed_forward(\n",
    "    x: np.array, W1: np.array, W2: np.array, b1: np.array, b2: np.array\n",
    ") -> np.array:\n",
    "    return relu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "\n",
    "def layer_norm(x: np.array, epsilon: float = 1e-6) -> np.array:\n",
    "    mean = x.mean(axis=1, keepdims=True)\n",
    "    # var = x.var(axis=1, keepdims=True)  # NOTE: variance is the standard deviation squared\n",
    "    # To lower the amount of calcuations (avoid calc. var and then sqrt in the denominator) we calculate the std dev\n",
    "    std_dev = x.std(axis=1, keepdims=True)\n",
    "    return (x - mean) / std_dev + epsilon * gamma + beta\n",
    "\n",
    "\n",
    "def encoder_decoder_attention(\n",
    "    attention_input: np.array,\n",
    "    encoder_output: np.array,\n",
    "    WQ: np.array,\n",
    "    WK: np.array,\n",
    "    WV: np.array,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    attention_input: the output of the previous attention!\n",
    "    \"\"\"\n",
    "    K = encoder_output @ WK\n",
    "    V = encoder_output @ WV\n",
    "    Q = attention_input @ WQ\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V\n",
    "\n",
    "\n",
    "def multihead_encoder_decoder_attention(\n",
    "    encoder_output: np.array,\n",
    "    attention_input: np.array,\n",
    "    WQs: np.array,\n",
    "    WKs: np.array,\n",
    "    WVs: np.array,\n",
    ") -> np.array:\n",
    "    attentions = np.concatenate(\n",
    "        [\n",
    "            encoder_decoder_attention(attention_input, encoder_output, WQ, WK, WV)\n",
    "            for WQ, WK, WV in zip(WQs, WKs, WVs)\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "\n",
    "def decoder_block(\n",
    "    x,\n",
    "    encoder_output,\n",
    "    WQs_self_attention,\n",
    "    WKs_self_attention,\n",
    "    WVs_self_attention,\n",
    "    WQs_ed_attention,\n",
    "    WKs_ed_attention,\n",
    "    WVs_ed_attention,\n",
    "    W1,\n",
    "    b1,\n",
    "    W2,\n",
    "    b2,\n",
    ") -> np.array:\n",
    "    Z = multihead_attention(\n",
    "        x, WQs_self_attention, WKs_self_attention, WVs_self_attention\n",
    "    )\n",
    "    Z = layer_norm(x + Z)  # LayerNorm of the Residual on attention output\n",
    "\n",
    "    enc_dec_attn = multihead_encoder_decoder_attention(\n",
    "        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention\n",
    "    )\n",
    "    enc_dec_attn = layer_norm(enc_dec_attn + Z)\n",
    "\n",
    "    output = feed_forward(enc_dec_attn, W1, W2, b1, b2)\n",
    "\n",
    "    return layer_norm(output + enc_dec_attn)  # LayerNorm of the Residual on FFN output\n",
    "\n",
    "\n",
    "def random_decoder_block(x: np.array, encoder_output: np.array) -> np.array:\n",
    "    WQs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs_self_attention = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    WQs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs_ed_attention = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "\n",
    "    return decoder_block(\n",
    "        x,\n",
    "        encoder_output,\n",
    "        WQs_self_attention,\n",
    "        WKs_self_attention,\n",
    "        WVs_self_attention,\n",
    "        WQs_ed_attention,\n",
    "        WKs_ed_attention,\n",
    "        WVs_ed_attention,\n",
    "        W1,\n",
    "        b1,\n",
    "        W2,\n",
    "        b2,\n",
    "    )\n",
    "\n",
    "\n",
    "def decoder(x, decoder_embedding, n=6) -> np.array:\n",
    "    for _ in range(n):\n",
    "        x = random_decoder_block(x, decoder_embedding)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.03733016,  0.43541475,  1.43082842, -0.82890901],\n",
       "       [-1.03733416,  0.43540516,  1.43083342, -0.82890042]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]], dtype=np.float64)\n",
    "E = np.random.randn(2, 4)\n",
    "\n",
    "# NOTE: He is getting two rows while I am getting only one. What the fuck is going on? I get as many rows as there are in E. Should this happen? \n",
    "decoder(E, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34707193, -0.6623798 , -1.14447372,  1.45978559]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = np.array([[1, 0, 0, 0]], dtype=np.float64)\n",
    "E += positional_encoding(E)\n",
    "\n",
    "encoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]], dtype=np.float64)\n",
    "\n",
    "decoder(E, decoder_embedding=encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
