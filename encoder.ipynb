{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence: str = \"Hello World\"\n",
    "# The original transformer method used a vector of size 512, I will do 4.\n",
    "# The embedding is random for now and I will hardcode a matrix. I might apply word2vec or GloVe later.\n",
    "E = np.array([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5]\n",
    "], dtype=np.float64)\n",
    "d_model = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embeddings: np.array, verbose: bool = False) -> np.array:\n",
    "    mat = np.zeros_like(a=embeddings, dtype=embeddings.dtype)\n",
    "    for pos, embedding in enumerate(embeddings):\n",
    "        if verbose:\n",
    "            print(f\"\\nEmbedding {embedding}\")\n",
    "        # for i, _ in enumerate(embedding): \n",
    "        # Pos encoding should be word agnostic and only looking at the \n",
    "        # position in the embedding matrix pos and dimension inside each embedding i.\n",
    "        for i in range(d_model):\n",
    "            func = np.sin if not i % 2 else np.cos\n",
    "            mat[pos][i] = func(pos / 10000 ** ((2 * i) / d_model))\n",
    "\n",
    "            # Copilot generated print horror for sanity check\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"i = {i} ({'even' if i % 2 == 0 else 'odd'}): PE({pos},{i}) = sin({pos} / 10000^({2 * i} / {d_model})) = sin({pos / 10000 ** ((2 * i) / d_model)}) = {func(pos / 10000 ** ((2 * i) / d_model))}\"\n",
    "                )\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 1.        ],\n",
       "       [0.84147098, 0.99995   , 0.0001    , 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encodings = positional_encoding(E)\n",
    "positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding positional encoding and input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E before adding:\n",
      "[[1. 2. 3. 4.]\n",
      " [2. 3. 4. 5.]]\n",
      "\n",
      "Positional encodings:\n",
      "[[0.         1.         0.         1.        ]\n",
      " [0.84147098 0.99995    0.0001     1.        ]]\n",
      "\n",
      "E after adding:\n",
      "[[1.         3.         3.         5.        ]\n",
      " [2.84147098 3.99995    4.0001     6.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"E before adding:\\n{E}\")\n",
    "print(f\"\\nPositional encodings:\\n{positional_encodings}\")\n",
    "\n",
    "E = E + positional_encodings\n",
    "\n",
    "print(f\"\\nE after adding:\\n{E}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention\n",
    "Let’s use 2 attention heads for our example. We’ll use random values for these matrices. Each matrix will be a 4x3 matrix. \n",
    "\n",
    "With this, each matrix will transform the 4-dimensional embeddings into 3-dimensional keys, values, and queries. \n",
    "\n",
    "This reduces the dimensionality for attention mechanism, which helps in managing the computational complexity. \n",
    "\n",
    "Note that using a too small attention size will hurt the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "WK1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WV1 = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1], [0, 1, 0]])\n",
    "WQ1 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]])\n",
    "\n",
    "WK2 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 1, 0]])\n",
    "WV2 = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]])\n",
    "WQ2 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# We need to multiply our embeddings with the weight matrices to obtain the keys, queries and values.\n",
    "K1 = E @ WK1    # Key calculations\n",
    "V1 = E @ WV1    # Values calculations\n",
    "Q1 = E @ WQ1    # Query calculations\n",
    "\n",
    "K2 = E @ WK2\n",
    "V2 = E @ WV2\n",
    "Q2 = E @ WQ2\n",
    "print(f\"Shape {K1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention function used in the paper is computed as:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ is the matrix of queries\n",
    "- $K$ is the matrix of keys\n",
    "- $V$ is the matrix of values\n",
    "- $d_k$ is the dimension of the keys\n",
    "\n",
    "\n",
    "Calculating the attention score requires a couple of steps:\n",
    "\n",
    "1. Calculate the dot product of the query with each key\n",
    "2. Divide the result by the square root of the dimension of the key vector\n",
    "3. Apply a softmax function to obtain the attention weights\n",
    "4. Multiply each value vector by the attention weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68.        , 105.25713083],\n",
       "       [ 87.9998    , 135.78163588]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "scores1 = Q1 @ K1.T\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.2598183 , 60.77023282],\n",
       "       [50.80670822, 78.39356402]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "scores1 = scores1 / np.sqrt(K1.shape[1])\n",
    "scores1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is computed as:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is the i-th element of the input vector\n",
    "- The denominator is the sum of the exponentials of all elements in the input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3\n",
    "def softmax(mat: np.array) -> np.array:\n",
    "    return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n",
    "\n",
    "scores1 = softmax(scores1)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reminding myself about the axis dilemma\n",
    "mat = np.array([[1, 2, 3], [2, 3, 4]])\n",
    "print(mat)\n",
    "np.sum(mat, axis=None)\n",
    "# Axis 1 means that the second dimension of the shape of the matrix is going to be collapsed and the number in that direction will be summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.00005   , 8.84147098, 6.84157098],\n",
       "       [8.00005   , 8.84147098, 6.84157098]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4\n",
    "scores1 = scores1 @ V1\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(E: np.array, WQ: np.array, WK: np.array, WV: np.array) -> np.array:\n",
    "    Q = E @ WQ\n",
    "    K = E @ WK\n",
    "    V = E @ WV\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.00004391, 8.84146233, 6.84156233],\n",
       "       [8.00004981, 8.84147071, 6.84157071]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = attention(E, WQ1, WQ1, WV1)\n",
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.84147098, 3.99995   , 8.00005   ],\n",
       "       [8.84147098, 3.99995   , 8.00005   ]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we get the attention score for the second attention head.\n",
    "score2 = attention(E, WQ2, WQ2, WV2)\n",
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.00005   , 8.84147098, 6.84157098, 8.84147098, 3.99995   ,\n",
       "        8.00005   ],\n",
       "       [8.00005   , 8.84147098, 6.84157098, 8.84147098, 3.99995   ,\n",
       "        8.00005   ]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.concatenate((scores1, score2), axis=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally multiply this concatenated matrix by a weight matrix to obtain the final output of the attention layer. \n",
    "\n",
    "This weight matrix is also learned! \n",
    "\n",
    "The dimension of the matrix ensures we go back to the same dimension as the embedding (4 in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.97128599, -14.12917589, -12.49224156, -18.50167966],\n",
       "       [ 11.971286  , -14.12917589, -12.49224156, -18.50167966]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just some random values\n",
    "W = np.array(\n",
    "    [\n",
    "        [0.79445237, 0.1081456, 0.27411536, 0.78394531],\n",
    "        [0.29081936, -0.36187258, -0.32312791, -0.48530339],\n",
    "        [-0.36702934, -0.76471963, -0.88058366, -1.73713022],\n",
    "        [-0.02305587, -0.64315981, -0.68306653, -1.25393866],\n",
    "        [0.29077448, -0.04121674, 0.01509932, 0.13149906],\n",
    "        [0.57451867, -0.08895355, 0.02190485, 0.24535932],\n",
    "    ]\n",
    ")\n",
    "Z = scores @ W\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward layer\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "\n",
    "connected feed-forward network, which is applied to each position separately and identically. \n",
    "\n",
    "\n",
    "This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "The feed forward layer, denoted as FFN, is computed as:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input\n",
    "- $W_1$ and $W_2$ are the weight matrices\n",
    "- $b_1$ and $b_2$ are the bias vectors\n",
    "- $\\max(0, \\cdot)$ is the ReLU (Rectified Linear Unit) activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this approach, I will expand from 4 dimensions to 8 in the first layer and then back to 4 in the second. \n",
    "d_model = 8\n",
    "W1 = np.random.randn(4, d_model)\n",
    "b1 = np.random.randn(d_model)\n",
    "W2 = np.random.randn(d_model, 4)\n",
    "b2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.array) -> np.array:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def feed_forward(x: np.array) -> np.array:\n",
    "    return relu(x @ W1 + b1) @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.27537029,  5.73870724, 44.73625853, 33.08235132],\n",
       "       [21.2753703 ,  5.73870724, 44.73625853, 33.08235133]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encapsulating everything so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 4\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "\n",
    "def softmax(mat: np.array) -> np.array:\n",
    "    return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n",
    "\n",
    "def attention(E: np.array, WQ: np.array, WK: np.array, WV: np.array) -> np.array:\n",
    "    Q = E @ WQ\n",
    "    K = E @ WK\n",
    "    V = E @ WV\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V\n",
    "\n",
    "def multihead_attention(x, WQs, WKs, WVs) -> np.array:\n",
    "    attentions = np.concatenate([attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1)\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "def relu(x: np.array) -> np.array:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def feed_forward(x: np.array, W1: np.array, W2: np.array, b1: np.array, b2: np.array) -> np.array:\n",
    "    return relu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "def encoder_block_(x: np.array, WQs: np.array, WKs: np.array, WVs: np.array, W1: np.array, W2: np.array, b1: np.array, b2: np.array) -> np.array:\n",
    "    Z = multihead_attention(x, WQs, WKs, WVs)\n",
    "    Z = feed_forward(Z, W1, W2, b1, b2)\n",
    "    return Z\n",
    "\n",
    "def random_encoder_block(x: np.array) -> np.array:\n",
    "    WQs = [\n",
    "        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WKs = [\n",
    "        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "    WVs = [\n",
    "        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "    ]\n",
    "\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "\n",
    "    return encoder_block(x, WQs, WKs, WVs, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 3.        , 3.        , 5.        ],\n",
       "       [2.84147098, 3.99995   , 4.0001    , 6.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.65114892, -3.74230864, -1.65071085, -1.83249781],\n",
       "       [ 5.65115293, -3.74231196, -1.6507186 , -1.83251494]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_encoder_block(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just one encoder block. The original paper uses 6 encoders. The output of one encoder goes to the next, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x: np.array):\n",
    "    for _ in range(6):\n",
    "        x = random_encoder_block(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_232297/1530465706.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n",
      "/tmp/ipykernel_232297/1530465706.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting too big numbers thus we need to use normalization.\n",
    "\n",
    "There are two common techniques to mitigate this problem: \n",
    "- residual connections\n",
    "- layer normalization. \n",
    "\n",
    "\n",
    "Residual connections:\n",
    "$$\\text{Residual}(x) = x + \\text{Layer}(x)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ is the input\n",
    "- $\\text{Layer}(x)$ is the output of the sublayer with x as the input \n",
    "\n",
    "Residual connections are simply adding the input of the layer to it output. \n",
    "\n",
    "For example, we add the initial embedding to the output of the attention. \n",
    "\n",
    "Residual connections mitigate the **vanishing gradient problem**. \n",
    "\n",
    "The intuition is that if the gradient is too small, we can just add the input to the output and the **gradient will be larger**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization, denoted as LayerNorm, is computed as:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\frac{x - \\mu(x)}{\\sqrt{\\sigma^2(x) + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input (embedding)\n",
    "- $\\mu(x)$ is the mean of $x$\n",
    "- $\\sigma^2(x)$ is the variance (squared standard deviation) of $x$\n",
    "- $\\epsilon$ is a small constant for numerical stability (usually $1e-6$)\n",
    "- $\\gamma$ and $\\beta$ are learnable parameters\n",
    "\n",
    "\n",
    "Layer normalization is a technique to normalize the inputs of a layer. \n",
    "\n",
    "It normalizes across the embedding dimension. \n",
    "\n",
    "The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1. \n",
    "\n",
    "This helps with the gradient flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike batch normalization (no worries if you don’t know what it is), **layer normalization normalizes** \n",
    "\n",
    "**across the embedding dimension** - that means that **each embedding will not be affected by other samples in the batch**.\n",
    "\n",
    "Why do we add the learnable parameters $\\gamma$ and $\\beta$ ? The reason is that we don’t want to lose the representational power of the layer. \n",
    "\n",
    "If we just normalize the inputs, we might lose some information. By adding the learnable parameters, we can learn to scale and shift the normalized values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our formulas for the encoder block (a single one) should look like this:\n",
    "\n",
    "1. The self-attention layer with residual connection and layer normalization, denoted as $Z(x)$, is computed as:\n",
    "\n",
    "    $$Z(x) = \\text{LayerNorm}(x + \\text{Attention}(x))$$\n",
    "\n",
    "2. The feed forward layer, denoted as FFN, is computed as:\n",
    "\n",
    "    $$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "3. The encoder layer, which combines the self-attention and feed forward layers with residual connections and layer normalization, is computed as:\n",
    "\n",
    "    $$\\text{Encoder}(x) = \\text{LayerNorm}(Z(x) + \\text{FFN}(Z(x) + x))$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input\n",
    "- $W_1$ and $W_2$ are the weight matrices\n",
    "- $b_1$ and $b_2$ are the bias vectors\n",
    "- $\\text{ReLU}(\\cdot)$ is the Rectified Linear Unit activation function\n",
    "- $\\text{LayerNorm}(\\cdot)$ is the layer normalization function\n",
    "- $\\text{Attention}(\\cdot)$ is the self-attention function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now calculate the layer normalization, we can divide it into three steps:\n",
    "\n",
    "1. Compute mean and variance for each embedding.\n",
    "2. Normalize by substracting the mean of its row and dividing by the square root of its row variance (plus a small number to avoid division by zero).\n",
    "3. Scale and shift by multiplying by gamma and adding beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-28.51127031],\n",
       "       [-27.33706259]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Z(x) = LayerNorm(x + Attention(x))\n",
    "# FFN(x) = ReLU(xW1 + b1)W2 + b2\n",
    "# Encoder(x) = LayerNorm(Z(x) + FFN(Z(x) + x))\n",
    "\n",
    "# 1. Compute mean and variance for EACH EMBEDDING (across the embedding dimension, i.e. axis=1)\n",
    "WQs = [\n",
    "    np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n",
    "]\n",
    "WKs = [\n",
    "    np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n",
    "]\n",
    "WVs = [\n",
    "    np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n",
    "]\n",
    "\n",
    "W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "b1 = np.random.randn(d_feed_forward)\n",
    "W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "b2 = np.random.randn(d_embedding)\n",
    "\n",
    "attn = multihead_attention(E, WQs, WKs, WVs)\n",
    "# Z(x) = LayerNorm(x + Attention(x)) -> LayerNorm(E + attn(E))\n",
    "# 1.1 mean of E+attn(E) accross the embedding dimension\n",
    "mean = (E + attn).mean(axis=1, keepdims=True)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2268.1795805],\n",
       "       [2311.7727231]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = (E + attn).var(axis=1, keepdims=True)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 51.8491194 , -36.07429372, -70.07217858, -57.35310153],\n",
       "       [ 53.79726931, -35.15201909, -69.21674721, -56.50249777]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming for simplification\n",
    "epsilon = 0.000001\n",
    "gamma = 1\n",
    "beta = 0\n",
    "norm = (E + attn) - mean / np.sqrt(var + epsilon) * gamma + beta\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x: np.array, epsilon: float=1e-6) -> np.array:\n",
    "    mean = x.mean(axis=1, keepdims=True)\n",
    "    var = x.var(axis=1, keepdims=True)\n",
    "    return x - mean / np.sqrt(var + epsilon) * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True]\n",
      " [ True  True  True  True]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 51.8491194 , -36.07429372, -70.07217858, -57.35310153],\n",
       "       [ 53.79726931, -35.15201909, -69.21674721, -56.50249777]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(layer_norm(E + attn) == norm)\n",
    "layer_norm(E + attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 4\n",
    "d_key = d_value = d_query = 4\n",
    "d_feed_forward = 8\n",
    "n_attention_heads = 2\n",
    "epsilon = 1e-6\n",
    "gamma = 1\n",
    "beta = 0\n",
    "\n",
    "def softmax(mat: np.array) -> np.array:\n",
    "    return np.exp(mat) / np.sum(a=np.exp(mat), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def attention(E: np.array, WQ: np.array, WK: np.array, WV: np.array) -> np.array:\n",
    "    Q = E @ WQ\n",
    "    K = E @ WK\n",
    "    V = E @ WV\n",
    "\n",
    "    return softmax((Q @ K.T) / np.sqrt(K.shape[1])) @ V\n",
    "\n",
    "\n",
    "def multihead_attention(x, WQs, WKs, WVs) -> np.array:\n",
    "    attentions = np.concatenate(\n",
    "        [attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n",
    "    )\n",
    "    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n",
    "    return attentions @ W\n",
    "\n",
    "\n",
    "def relu(x: np.array) -> np.array:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def feed_forward(\n",
    "    x: np.array, W1: np.array, W2: np.array, b1: np.array, b2: np.array\n",
    ") -> np.array:\n",
    "    return relu(x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "\n",
    "def layer_norm(x: np.array, epsilon: float=1e-6) -> np.array:\n",
    "    mean = x.mean(axis=1, keepdims=True)\n",
    "    # var = x.var(axis=1, keepdims=True)  # NOTE: variance is the standard deviation squared\n",
    "    # To lower the amount of calcuations (avoid calc. var and then sqrt in the denominator) we calculate the std dev\n",
    "    std_dev = x.std(axis=1, keepdims=True)\n",
    "    return (x - mean) / std_dev + epsilon * gamma + beta\n",
    "\n",
    "# New definition with layer normalization and residual connections\n",
    "def encoder_block(\n",
    "    x: np.array,\n",
    "    WQs: np.array,\n",
    "    WKs: np.array,\n",
    "    WVs: np.array,\n",
    "    W1: np.array,\n",
    "    W2: np.array,\n",
    "    b1: np.array,\n",
    "    b2: np.array,\n",
    ") -> np.array:\n",
    "    Z = multihead_attention(x, WQs, WKs, WVs)\n",
    "    Z = layer_norm(x + Z)  # LayerNorm of the Residual on attention output\n",
    "    output = feed_forward(Z, W1, W2, b1, b2)\n",
    "\n",
    "    return layer_norm(Z + output)  # LayerNorm of the Residual on FFN output\n",
    "\n",
    "\n",
    "def random_encoder_block(x: np.array) -> np.array:\n",
    "    WQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\n",
    "    WKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\n",
    "    WVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n",
    "\n",
    "    W1 = np.random.randn(d_embedding, d_feed_forward)\n",
    "    b1 = np.random.randn(d_feed_forward)\n",
    "    W2 = np.random.randn(d_feed_forward, d_embedding)\n",
    "    b2 = np.random.randn(d_embedding)\n",
    "\n",
    "    return encoder_block(x, WQs, WKs, WVs, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.79123393,  1.40275105, -1.08654597,  0.47503285],\n",
       "       [-0.79123088,  1.40275248, -1.08654759,  0.47502999]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encoder(x, n=6):\n",
    "    for _ in range(n):\n",
    "        x = random_encoder_block(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! These values make sense and we don’t get NaNs! \n",
    "\n",
    "The idea of the stack of encoders is that they output a continuous representation, z, \n",
    "that captures the meaning of the input sequence. \n",
    "\n",
    "This representation is then passed to the decoder, which will genrate an output sequence of symbols, one element at a time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
